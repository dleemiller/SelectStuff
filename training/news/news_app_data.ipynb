{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a308a81d-d1a6-4125-8478-6944eaa7d640",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Select the model to use as teacher and student during prompt optimization.\n",
    "\n",
    "DSPy uses the litellm strings (eg. **ollama_chat/...**). \n",
    "\n",
    "For optimization, you would typically chose the stronger model as a teacher, for proposing instructions and generating bootstrapped samples. The student model is the model you intend to use during the task. These could also be the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f9a20-b727-40c8-b36a-9bf9eb4d628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dspy\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# set your api key (if needed)\n",
    "load_dotenv(\"../../.env\")\n",
    "APIKEY = os.getenv(\"APIKEY\")\n",
    "\n",
    "# set your model (litellm model strings)\n",
    "model_id = \"openrouter/deepseek/deepseek-chat\"\n",
    "lm = dspy.LM(model_id, api_key=APIKEY, cache=True)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd552a-ad8a-4730-bb39-4cc6c07a9095",
   "metadata": {},
   "source": [
    "# Signatures\n",
    "\n",
    "Signatures are like DSPy's pydantic models. Describe the fields and docstrings as though they are prompts (they are).\n",
    "\n",
    "They will likely reflect the data in your table schema, but also could additional intermediate data structures in multi-hop patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9022cea-8a98-4673-8db6-85d1b61f4de5",
   "metadata": {},
   "source": [
    "### Initial prototype\n",
    "```python\n",
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "class NewsAppSignatureExample(dspy.Signature):\n",
    "    text: str = dspy.InputField(desc=\"Text from an article for analysis\")\n",
    "    category: Literal[\"world\", \"entertainment\", \"science\", \"health\", \"business\", \"sports\", \"politics\", \"tech\"] = dspy.OutputField(desc=\"Article content category\")\n",
    "    title: str = dspy.OutputField(desc=\"Article title, when available. Otherwise create one\")\n",
    "    tags: list[str] = dspy.OutputField(desc=\"Tags for search and classification\")\n",
    "    notable_people: Optional[list[str]] = dspy.OutputField(desc=\"Names of notable people in the article\")\n",
    "    notable_organizations: Optional[list[str]] = dspy.OutputField(desc=\"Names of notable organizations in the article\")\n",
    "\n",
    "\n",
    "# system prompt goes in the docstring\n",
    "NewsAppSignatureExample.__doc__ = \"\"\"\n",
    "You are provided with the text of a news article. Help provide the requested information for catalogging.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037e3db-8f6e-4173-b5b7-2fc6df0c979a",
   "metadata": {},
   "source": [
    "With some good examples in hand, I refined an expanded list with ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fea67-2f38-4a63-acb6-f14580d1d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_app import NewsAppSignature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cf27a-ab1e-4d04-b478-5dbd1fbd0b3f",
   "metadata": {},
   "source": [
    "# Run the program\n",
    "\n",
    "I like the natural code style of writing a DSPy signature. A pydantic model becomes the prompt.\n",
    "\n",
    "`Literal` type + LLM = classifier (cool!)\n",
    "\n",
    "We can already try it out, using the ChainOfThought predictor to run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb03ce3-c352-4510-91b3-e600a885cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Business Briefing Dec. 2, 2015\n",
    "Nokia shareholders overwhelmingly approved the acquisition of the ailing French telecom Alcatel-Lucent, removing one of the last hurdles to a 15.6 billion euro ($16.5 billion) deal that will make Nokia a market leader in networks.\n",
    "In October, Nokia said it would pay 4 billion to shareholders as the company raised its outlook for the year.\n",
    "Rajeev Suri, Nokias chief executive, said he was delighted by shareholders recognizing the long-term value creation opportunity of the deal, which is expected to close during the first quarter of 2016.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a08ef1-b89a-43a9-a184-7397b298dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dspy.ChainOfThought(NewsAppSignature)\n",
    "catalog_item = catalog(article_text=text)\n",
    "print(catalog_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfcf17-a0dd-43b4-94e1-534769e3ecb2",
   "metadata": {},
   "source": [
    "# Generating training data\n",
    "\n",
    "We'll rely on \"best of n\" scaling to help create synthetic data for our application. Then we'll manually review ~100 examples we created for training.\n",
    "\n",
    "\n",
    "## A basic test time scaling\n",
    "\n",
    "I'll generate some training data using a simplistic best-of-n style test time scaling. Aggregating all of the types is a bit more challenging, so I've done that in the `aggregate/` folder as a module that I can work on further.\n",
    "\n",
    "Depending on where you are running your LLM calls, you might choose the serial or parallel methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fca1d4-995b-4e23-8cae-f95cf0bff7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import Parallel, ChainOfThought\n",
    "from typing import List, Literal\n",
    "import tqdm\n",
    "\n",
    "def generate_candidates_serial(text, n=8):\n",
    "    \"\"\" Run in serial \"\"\"\n",
    "    return [catalog(article_text=text) for _ in range(n)]\n",
    "\n",
    "\n",
    "def generate_candidates_parallel(text, n=8, num_threads=2):\n",
    "    \"\"\" Run in parallel \"\"\"\n",
    "    parallel_executor = dspy.Parallel(num_threads=num_threads)\n",
    "    exec_pairs = [(catalog, {'article_text': text}) for _ in range(n)]\n",
    "    results = parallel_executor.forward(exec_pairs)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd902b8d-a9c6-435f-a532-11a0fea6090b",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "We need to aggregate by each field to obtain consensus results. For lists, we fuzzy deduplicate and then set a threshold for N minimum occurrences for acceptence. We are targeting aggregation from 8 outputs.\n",
    "\n",
    "I've modularized the code and imported it here, since it's a bit long and not especially interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35393f63-d9ee-4b6f-aab7-da90e213d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from aggregate.aggregate import LLMOutputAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6798a-765f-4bc0-9e55-dbe321970efe",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import List, Optional, Literal, Dict, Any, Union\n",
    "from collections import Counter\n",
    "import textdistance\n",
    "import itertools\n",
    "from pydantic import ValidationError\n",
    "from typing import get_origin, get_args, Union\n",
    "\n",
    "def is_optional_field(type_hint) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if a type hint is Optional, i.e., Union[X, None].\n",
    "    \"\"\"\n",
    "    return get_origin(type_hint) is Union and type(None) in get_args(type_hint)\n",
    "\n",
    "def aggregate_signatures(text, predictions: List[Any], threshold: int = 2, debug: bool = False) -> NewsAppSignature:\n",
    "    \"\"\"\n",
    "    Aggregates multiple Prediction objects into a single NewsAppSignature.\n",
    "    \n",
    "    Args:\n",
    "        predictions (List[Any]): A list of Prediction objects.\n",
    "        threshold (int): Minimum number of occurrences for a cluster to be included.\n",
    "    \n",
    "    Returns:\n",
    "        NewsAppSignature: The aggregated signature.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If required fields are missing or validation fails.\n",
    "    \"\"\"\n",
    "    if not predictions:\n",
    "        raise ValueError(\"No predictions to aggregate.\")\n",
    "\n",
    "    aggregated_fields: Dict[str, Any] = {}\n",
    "    \n",
    "    # Helper function for majority voting\n",
    "    def majority_vote(values: List[Any]) -> Any:\n",
    "        counter = Counter(values)\n",
    "        if debug:\n",
    "            print(counter)\n",
    "        most_common, count = counter.most_common(1)[0]\n",
    "        return most_common\n",
    "\n",
    "    # Helper function for clustering similar strings with frequency threshold\n",
    "    def cluster_strings_with_threshold(strings: List[str], threshold: int, similarity_threshold: float = 0.6) -> List[str]:\n",
    "        \"\"\"\n",
    "        Clusters similar strings based on Jaccard similarity and filters clusters based on frequency threshold.\n",
    "        \n",
    "        Args:\n",
    "            strings (List[str]): List of strings to cluster.\n",
    "            threshold (int): Minimum number of occurrences for a cluster to be included.\n",
    "            similarity_threshold (float): Jaccard similarity threshold for clustering.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of representative strings from clusters that meet the threshold.\n",
    "        \"\"\"\n",
    "        clusters = []\n",
    "        for string in strings:\n",
    "            added = False\n",
    "            for cluster in clusters:\n",
    "                # Compare with the first item in the cluster\n",
    "                similarity = textdistance.jaccard.normalized_similarity(\n",
    "                    set(string.lower().split()), set(cluster[0].lower().split())\n",
    "                )\n",
    "                if similarity >= similarity_threshold:\n",
    "                    cluster.append(string)\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                clusters.append([string])\n",
    "        \n",
    "        # Filter clusters based on threshold\n",
    "        if debug:\n",
    "            print(clusters)\n",
    "        filtered_clusters = [cluster for cluster in clusters if len(cluster) >= threshold]\n",
    "        \n",
    "        # Return one representative from each filtered cluster\n",
    "        return [cluster[0] for cluster in filtered_clusters]\n",
    "    \n",
    "    # Iterate over each field in the NewsAppSignature\n",
    "    for field_name, field_type in NewsAppSignature.__annotations__.items():\n",
    "        # Special handling for 'article_text' since it's turned off\n",
    "        if field_name == \"article_text\":\n",
    "            # Set 'article_text' to an empty string as per user's instruction\n",
    "            aggregated_fields[field_name] = text\n",
    "            continue\n",
    "\n",
    "        # Collect all non-None values for the current field\n",
    "        field_values = [getattr(pred, field_name, None) for pred in predictions]\n",
    "        field_values = [val for val in field_values if val is not None]\n",
    "\n",
    "        if not field_values:\n",
    "            # Determine if the field is optional\n",
    "            if is_optional_field(field_type):\n",
    "                aggregated_fields[field_name] = None\n",
    "            else:\n",
    "                # For required fields with no values, raise an error\n",
    "                raise ValueError(f\"No values found for required field '{field_name}' during aggregation.\")\n",
    "            continue\n",
    "\n",
    "        # Determine the field type for aggregation\n",
    "        origin_type = get_origin(field_type)\n",
    "        args_type = get_args(field_type)\n",
    "\n",
    "        # Handle Literal types\n",
    "        if origin_type is Literal:\n",
    "            # Majority voting for Literal fields\n",
    "            aggregated_fields[field_name] = majority_vote(field_values)\n",
    "        elif isinstance(field_values[0], str):\n",
    "            # Majority voting for single-string fields\n",
    "            aggregated_fields[field_name] = majority_vote(field_values)\n",
    "        elif isinstance(field_values[0], list):\n",
    "            # Flatten all lists\n",
    "            flattened = list(itertools.chain.from_iterable(field_values))\n",
    "            # Cluster similar strings with frequency threshold\n",
    "            clustered = cluster_strings_with_threshold(flattened, threshold=threshold)\n",
    "            aggregated_fields[field_name] = clustered\n",
    "        else:\n",
    "            # Handle other types if necessary\n",
    "            aggregated_fields[field_name] = majority_vote(field_values)\n",
    "\n",
    "    # Instantiate the aggregated NewsAppSignature with all fields\n",
    "    try:\n",
    "        aggregated_signature = NewsAppSignature(**aggregated_fields)\n",
    "    except ValidationError as ve:\n",
    "        # Extract detailed validation errors\n",
    "        raise ValueError(f\"Error creating aggregated NewsAppSignature: {ve}\")\n",
    "\n",
    "    return aggregated_signature\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319c68b-3229-4e5a-b46d-ce345d6fcd56",
   "metadata": {},
   "source": [
    "```python\n",
    "consensus = aggregate_signatures(text, results, debug=True, threshold=4)\n",
    "consensus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c8957-bdd1-4256-b115-24f975d87f47",
   "metadata": {},
   "source": [
    "## Process a bunch of data\n",
    "\n",
    "We can load `ag_news` to create our synthetic training data, and process ~100 rows.\n",
    "\n",
    "I'll save the save the results as I go. Quick and dirty, just restart if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7694045-7294-497f-bc2f-db4a39076e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a diverse news dataset (e.g., \"ag_news\")\n",
    "dataset = load_dataset(\"valurank/News_Articles_Categorization\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073ed2f-e6d5-447a-a1e7-797beb940add",
   "metadata": {},
   "source": [
    "### Utilities for tracking the dataset offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c166f1c-68fe-46a8-8a9f-5f00cac3716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "# Define the number of articles and samples\n",
    "num_articles = 100\n",
    "samples_per_article = 8\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"training_data\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define a file to keep track of progress (offset)\n",
    "progress_file = os.path.join(output_dir, \"progress.txt\")\n",
    "\n",
    "# Function to generate a non-cryptographic hash (e.g., MD5) of a JSON string\n",
    "def generate_hash(json_str: str) -> str:\n",
    "    return hashlib.md5(json_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Function to load the current offset\n",
    "def load_offset() -> int:\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            try:\n",
    "                offset = int(f.read().strip())\n",
    "                return offset\n",
    "            except ValueError:\n",
    "                return 0\n",
    "    return 0\n",
    "\n",
    "# Function to save the current offset\n",
    "def save_offset(offset: int):\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(str(offset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5e149-c4c6-48a5-836a-fb3c49759da6",
   "metadata": {},
   "source": [
    "### Best-Of-N Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c3542-83ae-4782-8a85-663ad055c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the starting offset\n",
    "start_offset = load_offset()\n",
    "\n",
    "# Iterate over the specified number of articles starting from the offset\n",
    "for i in tqdm.tqdm(range(start_offset, num_articles), desc=\"Processing Articles\", total=num_articles - start_offset):\n",
    "    try:\n",
    "        article = dataset[i]\n",
    "        text = article['Text']\n",
    "        \n",
    "        # Generate multiple predictions\n",
    "        # candidates = generate_candidates_serial(text, n=samples_per_article)\n",
    "        candidates = generate_candidates_parallel(text, n=samples_per_article)\n",
    "        \n",
    "        # Aggregate predictions to form consensus\n",
    "        candidates_with_text = []\n",
    "        for c in [c.toDict() for c in candidates]:\n",
    "            c.update({\"article_text\": text})\n",
    "            candidates_with_text.append(c)\n",
    "        candidates_with_text\n",
    "        consensus = LLMOutputAggregator.aggregate(\n",
    "            NewsAppSignature, candidates_with_text, threshold=3\n",
    "        )\n",
    "        \n",
    "        # Convert consensus to JSON string\n",
    "        consensus_json = consensus.model_dump_json()\n",
    "        \n",
    "        # Generate filename using hash of JSON string\n",
    "        filename_hash = generate_hash(consensus_json)\n",
    "        filename = f\"{filename_hash}.json\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the JSON string to the file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(consensus_json)\n",
    "        \n",
    "        # Update the progress offset\n",
    "        save_offset(i + 1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article {i}: {e}\")\n",
    "        # Optionally, log the error to a file\n",
    "        error_log = os.path.join(output_dir, \"error_log.txt\")\n",
    "        with open(error_log, 'a') as f:\n",
    "            f.write(f\"Article {i}: {e}\\n\")\n",
    "        # Continue with the next article\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc33d81-cb2f-445c-8570-4f1f475b946f",
   "metadata": {},
   "source": [
    "## Review data\n",
    "\n",
    "(This is done in the review tool.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d694e4c-92c0-46b5-8ee3-d509e3f2ba04",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f01875-1130-4652-a338-7376779c6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "data = []\n",
    "for file in glob.glob(\"training_data/accepted/*.json\"):\n",
    "    with open(file, \"r\") as fh:\n",
    "        tmp = json.load(fh)\n",
    "\n",
    "        # convert to date\n",
    "        tmp[\"publication_date\"] = datetime.strptime(tmp[\"publication_date\"], \"%Y-%m-%d\").date() if tmp[\"publication_date\"] else None\n",
    "\n",
    "        # remove reasoning from example\n",
    "        if \"reasoning\" in tmp:\n",
    "            del tmp[\"reasoning\"]\n",
    "\n",
    "        e = dspy.Example(tmp).with_inputs(\"article_text\")\n",
    "        data.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc498dd-27b4-4e18-9681-9c3e6e17f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626b178-4155-42ad-9c83-ad9aa28bc382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scorer\")\n",
    "from scorer import WordLlamaScorer\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "\n",
    "scorer = WordLlamaScorer.from_signature(NewsAppSignature, skip_fields=[\"article_text\", \"reasoning\"])\n",
    "\n",
    "\n",
    "teleprompter = MIPROv2(\n",
    "    metric=scorer,\n",
    "    auto=\"medium\",\n",
    "    teacher_settings=dict(lm=teacher_lm),\n",
    "    num_threads=2\n",
    ")\n",
    "\n",
    "catalog = dspy.ChainOfThought(NewsAppSignature)\n",
    "optimized_program = teleprompter.compile(\n",
    "    student=catalog.deepcopy(),\n",
    "    teacher=catalog.deepcopy(),\n",
    "    trainset=data,\n",
    "    max_bootstrapped_demos=2,\n",
    "    max_labeled_demos=2,\n",
    "    requires_permission_to_run=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6497f0-9e39-4746-861d-85ee736193f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo in optimized_program.demos:\n",
    "    if 'publication_date' in demo and isinstance(demo['publication_date'], date):\n",
    "        demo['publication_date'] = demo['publication_date'].isoformat()\n",
    "\n",
    "# Save the state to a JSON file\n",
    "optimized_program.save(\"miprov2_command_r7b.json\", save_program=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e8987-89d4-4e0d-a848-65cd43a9a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program(article_text=text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8a3c4-ebf5-461e-9896-ad197de83219",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9687a-e0ca-4a9f-b253-326414be9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordllama import WordLlama\n",
    "\n",
    "wl = WordLlama.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c2c5c-56eb-4095-9822-407170d54f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "data = []\n",
    "for file in glob.glob(\"training_data/accepted/*.json\"):\n",
    "    with open(file, \"r\") as fh:\n",
    "        data.append(json.load(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5aa998-860d-4add-bad3-ef4edb97370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [x[\"article_text\"] for x in data]\n",
    "\n",
    "embeds = wl.embed(texts)\n",
    "sim_matrix = wl.vector_similarity(embeds, embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4882dd6-e9f9-4022-9c98-5617ce439d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idx = np.concatenate([x[:, None] for x in np.where(sim_matrix > 0.9)], axis=1)\n",
    "deduplicate = list(filter(lambda x: x[0] < x[1], idx)) # lower triangle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0056bb9-c52e-44a8-994b-7ebd114aff3a",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def remove_older_file(file1_path: str, file2_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Compare two files and remove the older one.\n",
    "    Returns the path of the removed file.\n",
    "    \"\"\"\n",
    "    # Get modification timestamps for both files\n",
    "    time1 = os.path.getmtime(file1_path)\n",
    "    time2 = os.path.getmtime(file2_path)\n",
    "    \n",
    "    # Compare and remove older file\n",
    "    if time1 < time2:\n",
    "        os.remove(file1_path)\n",
    "        return file1_path\n",
    "    else:\n",
    "        os.remove(file2_path)\n",
    "        return file2_path\n",
    "\n",
    "\n",
    "files = list(glob.glob(\"training_data/accepted/*.json\"))\n",
    "for pair in deduplicate:\n",
    "    older_file = remove_older_file(files[pair[0]], files[pair[1]])\n",
    "    print(f\"Removed older file: {older_file}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9ca6e-b22d-4fc7-97fe-fe73c68379b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
