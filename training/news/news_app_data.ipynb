{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f9a20-b727-40c8-b36a-9bf9eb4d628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dspy\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# set your api key (if needed)\n",
    "load_dotenv(\"../../.env\")\n",
    "APIKEY = os.getenv(\"APIKEY\")\n",
    "\n",
    "# set your model (litellm model strings)\n",
    "#model_id = \"openrouter/deepseek/deepseek-chat\"\n",
    "model_id = \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n",
    "lm = dspy.LM(model_id, api_key=APIKEY, cache=False)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd552a-ad8a-4730-bb39-4cc6c07a9095",
   "metadata": {},
   "source": [
    "# Signatures\n",
    "\n",
    "Signatures are like DSPy's pydantic models. Describe the fields and docstrings as though they are prompts (they are).\n",
    "\n",
    "They will likely reflect the data in your table schema, but also could additional intermediate data structures in multi-hop patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9022cea-8a98-4673-8db6-85d1b61f4de5",
   "metadata": {},
   "source": [
    "### Initial prototype\n",
    "```python\n",
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "class NewsAppSignatureExample(dspy.Signature):\n",
    "    text: str = dspy.InputField(desc=\"Text from an article for analysis\")\n",
    "    category: Literal[\"world\", \"entertainment\", \"science\", \"health\", \"business\", \"sports\", \"politics\", \"tech\"] = dspy.OutputField(desc=\"Article content category\")\n",
    "    title: str = dspy.OutputField(desc=\"Article title, when available. Otherwise create one\")\n",
    "    tags: list[str] = dspy.OutputField(desc=\"Tags for search and classification\")\n",
    "    notable_people: Optional[list[str]] = dspy.OutputField(desc=\"Names of notable people in the article\")\n",
    "    notable_organizations: Optional[list[str]] = dspy.OutputField(desc=\"Names of notable organizations in the article\")\n",
    "\n",
    "\n",
    "# system prompt goes in the docstring\n",
    "NewsAppSignatureExample.__doc__ = \"\"\"\n",
    "You are provided with the text of a news article. Help provide the requested information for catalogging.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037e3db-8f6e-4173-b5b7-2fc6df0c979a",
   "metadata": {},
   "source": [
    "With some good examples in hand, I refined an expanded list with ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fea67-2f38-4a63-acb6-f14580d1d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_app import NewsAppSignature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cf27a-ab1e-4d04-b478-5dbd1fbd0b3f",
   "metadata": {},
   "source": [
    "# Run the program\n",
    "\n",
    "I like the natural code style of writing a DSPy signature. A pydantic model becomes the prompt.\n",
    "\n",
    "`Literal` type + LLM = classifier (cool!)\n",
    "\n",
    "We can already try it out, using the ChainOfThought predictor to run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb03ce3-c352-4510-91b3-e600a885cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Business Briefing Dec. 2, 2015\n",
    "Nokia shareholders overwhelmingly approved the acquisition of the ailing French telecom Alcatel-Lucent, removing one of the last hurdles to a 15.6 billion euro ($16.5 billion) deal that will make Nokia a market leader in networks.\n",
    "In October, Nokia said it would pay 4 billion to shareholders as the company raised its outlook for the year.\n",
    "Rajeev Suri, Nokias chief executive, said he was delighted by shareholders recognizing the long-term value creation opportunity of the deal, which is expected to close during the first quarter of 2016.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a08ef1-b89a-43a9-a184-7397b298dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dspy.ChainOfThought(NewsAppSignature)\n",
    "catalog_item = catalog(article_text=text)\n",
    "print(catalog_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfcf17-a0dd-43b4-94e1-534769e3ecb2",
   "metadata": {},
   "source": [
    "# Generating training data\n",
    "\n",
    "We'll rely on \"best of n\" scaling to help create synthetic data for our application. Then we'll manually review ~100 examples we created for training.\n",
    "\n",
    "\n",
    "## A basic test time scaling\n",
    "\n",
    "I'll generate some training data using a simplistic best-of-n style test time scaling. Aggregating all of the types is a bit more challenging, so I've done that in the `aggregate/` folder as a module that I can work on further.\n",
    "\n",
    "Depending on where you are running your LLM calls, you might choose the serial or parallel methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fca1d4-995b-4e23-8cae-f95cf0bff7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def generate_candidates_serial(text, n=8):\n",
    "    \"\"\"Run in serial\"\"\"\n",
    "    return [catalog(article_text=text) for _ in range(n)]\n",
    "\n",
    "\n",
    "def generate_candidates_parallel(text, n=8, num_threads=2):\n",
    "    \"\"\"Run in parallel\"\"\"\n",
    "    parallel_executor = dspy.Parallel(num_threads=num_threads)\n",
    "    exec_pairs = [(catalog, {\"article_text\": text}) for _ in range(n)]\n",
    "    results = parallel_executor.forward(exec_pairs)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd902b8d-a9c6-435f-a532-11a0fea6090b",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "We need to aggregate by each field to obtain consensus results. For lists, we fuzzy deduplicate and then set a threshold for N minimum occurrences for acceptence. We are targeting aggregation from 8 outputs.\n",
    "\n",
    "I've modularized the code and imported it here, since it's a bit long and not especially interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35393f63-d9ee-4b6f-aab7-da90e213d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from aggregate.aggregate import LLMOutputAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c8957-bdd1-4256-b115-24f975d87f47",
   "metadata": {},
   "source": [
    "## Process a bunch of data\n",
    "\n",
    "We can load `ag_news` to create our synthetic training data, and process ~100 rows.\n",
    "\n",
    "I'll save the save the results as I go. Quick and dirty, just restart if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7694045-7294-497f-bc2f-db4a39076e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a diverse news dataset (e.g., \"ag_news\")\n",
    "dataset = load_dataset(\"valurank/News_Articles_Categorization\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073ed2f-e6d5-447a-a1e7-797beb940add",
   "metadata": {},
   "source": [
    "### Utilities for tracking the dataset offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c166f1c-68fe-46a8-8a9f-5f00cac3716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "# Define the number of articles and samples\n",
    "num_articles = 100\n",
    "samples_per_article = 8\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"training_data\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define a file to keep track of progress (offset)\n",
    "progress_file = os.path.join(output_dir, \"progress.txt\")\n",
    "\n",
    "\n",
    "# Function to generate a non-cryptographic hash (e.g., MD5) of a JSON string\n",
    "def generate_hash(json_str: str) -> str:\n",
    "    return hashlib.md5(json_str.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "# Function to load the current offset\n",
    "def load_offset() -> int:\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, \"r\") as f:\n",
    "            try:\n",
    "                offset = int(f.read().strip())\n",
    "                return offset\n",
    "            except ValueError:\n",
    "                return 0\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Function to save the current offset\n",
    "def save_offset(offset: int):\n",
    "    with open(progress_file, \"w\") as f:\n",
    "        f.write(str(offset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5e149-c4c6-48a5-836a-fb3c49759da6",
   "metadata": {},
   "source": [
    "### Best-Of-N Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c3542-83ae-4782-8a85-663ad055c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the starting offset\n",
    "start_offset = load_offset()\n",
    "\n",
    "# Iterate over the specified number of articles starting from the offset\n",
    "for i in tqdm.tqdm(\n",
    "    range(start_offset, num_articles),\n",
    "    desc=\"Processing Articles\",\n",
    "    total=num_articles - start_offset,\n",
    "):\n",
    "    try:\n",
    "        article = dataset[i]\n",
    "        text = article[\"Text\"]\n",
    "\n",
    "        # Generate multiple predictions\n",
    "        # candidates = generate_candidates_serial(text, n=samples_per_article)\n",
    "        candidates = generate_candidates_parallel(text, n=samples_per_article)\n",
    "\n",
    "        # Aggregate predictions to form consensus\n",
    "        candidates_with_text = []\n",
    "        for c in [c.toDict() for c in candidates]:\n",
    "            c.update({\"article_text\": text})\n",
    "            candidates_with_text.append(c)\n",
    "        candidates_with_text\n",
    "        consensus = LLMOutputAggregator.aggregate(\n",
    "            NewsAppSignature, candidates_with_text, threshold=3\n",
    "        )\n",
    "\n",
    "        # Convert consensus to JSON string\n",
    "        consensus_json = consensus.model_dump_json()\n",
    "\n",
    "        # Generate filename using hash of JSON string\n",
    "        filename_hash = generate_hash(consensus_json)\n",
    "        filename = f\"{filename_hash}.json\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Save the JSON string to the file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(consensus_json)\n",
    "\n",
    "        # Update the progress offset\n",
    "        save_offset(i + 1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article {i}: {e}\")\n",
    "        # Optionally, log the error to a file\n",
    "        error_log = os.path.join(output_dir, \"error_log.txt\")\n",
    "        with open(error_log, \"a\") as f:\n",
    "            f.write(f\"Article {i}: {e}\\n\")\n",
    "        # Continue with the next article\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc33d81-cb2f-445c-8570-4f1f475b946f",
   "metadata": {},
   "source": [
    "## Review data\n",
    "\n",
    "(This is done in the review tool.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c4998-5654-43f0-a225-84714856426c",
   "metadata": {},
   "source": [
    "# Summary data\n",
    "\n",
    "1/27/25 - feature add summary to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5edad6-a24c-4764-a35e-bf7b243077a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "data = []\n",
    "for fn in glob.glob(\"./training_data/accepted/*.json\"):\n",
    "    with open(fn, \"r\") as fh:\n",
    "        data.append((fn, json.load(fh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77084028-c1c7-4753-b90f-2b38b7144ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestSnippet(dspy.Signature):\n",
    "    \"\"\" Choose the best snippet for the article \"\"\"\n",
    "    article_text: str = dspy.InputField(desc=\"Original article text\")\n",
    "    snippets: list[str] = dspy.InputField(desc=\"Generated snippets\")\n",
    "    best_snippet: str = dspy.OutputField(desc=\"Snippet that best works for the article\")\n",
    "\n",
    "\n",
    "class ArticleSummary(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.summary = dspy.Predict(\"article_text -> snippet\")\n",
    "        self.best = dspy.ChainOfThought(BestSnippet)\n",
    "\n",
    "    def forward(self, article_text: str, n: int = 8):\n",
    "        snippets = []\n",
    "        for i in range(n):\n",
    "            result = self.summary(article_text=article_text)\n",
    "            dspy.Suggest(\n",
    "                len(result.snippet) < 500,\n",
    "                \"Snippet is too long.\"\n",
    "            )\n",
    "            snippets.append(result.snippet)\n",
    "\n",
    "        selection = self.best(article_text=article_text, snippets=snippets)\n",
    "        return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e73cc-7d8a-425c-a8cf-c9040354bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "summarizer = ArticleSummary().activate_assertions()\n",
    "\n",
    "for fn, article in tqdm.tqdm(data, desc=\"Processing Articles\", total=len(data)):\n",
    "    base_name = os.path.split(fn)[-1]\n",
    "    outpath = f\"output/{base_name}\"\n",
    "    if os.path.exists(outpath):\n",
    "        continue\n",
    "    try:\n",
    "        result = summarizer(article[\"article_text\"])\n",
    "        output_text = result.best_snippet\n",
    "        article[\"snippet\"] = output_text.strip('\"')\n",
    "        with open(outpath, \"w\") as fh:\n",
    "            json.dump(article, fh)\n",
    "    except:\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227499ad-6224-4aa9-b0d9-80b27b6b864a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
