{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a308a81d-d1a6-4125-8478-6944eaa7d640",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Select the model to use as teacher and student during prompt optimization.\n",
    "\n",
    "DSPy uses the litellm strings (eg. **ollama_chat/...**). \n",
    "\n",
    "For optimization, you would typically chose the stronger model as a teacher, for proposing instructions and generating bootstrapped samples. The student model is the model you intend to use during the task. These could also be the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016f9a20-b727-40c8-b36a-9bf9eb4d628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dspy\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# set your api key (if needed)\n",
    "load_dotenv(\"../../.env\")\n",
    "APIKEY = os.getenv(\"APIKEY\")\n",
    "\n",
    "# set your model (litellm model strings)\n",
    "student_model = \"openrouter/deepseek/deepseek-chat\"\n",
    "teacher_model = \"openrouter/deepseek/deepseek-chat\"\n",
    "lm = dspy.LM(teacher_model, api_key=APIKEY, temperature=1.0, cache=False)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd552a-ad8a-4730-bb39-4cc6c07a9095",
   "metadata": {},
   "source": [
    "# Signatures\n",
    "\n",
    "Signatures are like DSPy's pydantic models. Describe the fields and docstrings as though they are prompts (they are).\n",
    "\n",
    "They will likely reflect the data in your table schema, but also could additional intermediate data structures in multi-hop patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9022cea-8a98-4673-8db6-85d1b61f4de5",
   "metadata": {},
   "source": [
    "### Initial prototype\n",
    "```python\n",
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "class NewsAppSignatureExample(dspy.Signature):\n",
    "    text: str = dspy.InputField(desc=\"Text from an article for analysis\")\n",
    "    category: Literal[\"world\", \"entertainment\", \"science\", \"health\", \"business\", \"sports\", \"politics\", \"tech\"] = dspy.OutputField(desc=\"Article content category\")\n",
    "    title: str = dspy.OutputField(desc=\"Article title, when available. Otherwise create one\")\n",
    "    tags: list[str] = dspy.OutputField(desc=\"Tags for search and classification\")\n",
    "    notable_people: Optional[list[str]] = dspy.OutputField(desc=\"Names of notable people in the article\")\n",
    "    notable_organizations: Optional[list[str]] = dspy.OutputField(desc=\"Names of notable organizations in the article\")\n",
    "\n",
    "\n",
    "# system prompt goes in the docstring\n",
    "NewsAppSignatureExample.__doc__ = \"\"\"\n",
    "You are provided with the text of a news article. Help provide the requested information for catalogging.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037e3db-8f6e-4173-b5b7-2fc6df0c979a",
   "metadata": {},
   "source": [
    "With some good examples in hand, I refined an expanded list with ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0fea67-2f38-4a63-acb6-f14580d1d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, List\n",
    "\n",
    "\n",
    "class NewsAppSignature(dspy.Signature):\n",
    "    # INPUT FIELD\n",
    "    article_text: str = dspy.InputField(\n",
    "        desc=\"Full article text for analysis.\"\n",
    "    )\n",
    "    \n",
    "    # BASIC IDENTIFIERS\n",
    "    generated_title: str = dspy.OutputField(\n",
    "        desc=\"Article title or a generated one if not found.\"\n",
    "    )\n",
    "    \n",
    "    publication_date: Optional[str] = dspy.OutputField(\n",
    "        desc=\"Publication date in YYYY-MM-DD format.\"\n",
    "    )\n",
    "    \n",
    "    # CATEGORIZATION\n",
    "    primary_category: Literal[\n",
    "        \"world\",\n",
    "        \"entertainment\",\n",
    "        \"science\",\n",
    "        \"health\",\n",
    "        \"business\",\n",
    "        \"sports\",\n",
    "        \"politics\",\n",
    "        \"technology\",\n",
    "        \"legal\",\n",
    "        \"community\",\n",
    "        \"public_safety\"\n",
    "    ] = dspy.OutputField(\n",
    "        desc=\"Primary subject category of the article.\"\n",
    "    )\n",
    "    \n",
    "    content_type: Literal[\n",
    "        \"editorial\",\n",
    "        \"opinion\",\n",
    "        \"analysis\",\n",
    "        \"reporting\",\n",
    "        \"interview\",\n",
    "        \"investigative\",\n",
    "        \"press_release\",\n",
    "        \"blog_post\"\n",
    "    ] = dspy.OutputField(\n",
    "        desc=\"Type of article content.\"\n",
    "    )\n",
    "    \n",
    "    keywords: List[str] = dspy.OutputField(\n",
    "        desc=\"Keywords or phrases for classification and retrieval.\"\n",
    "    )\n",
    "    \n",
    "    # ENTITY MENTIONS\n",
    "    mentioned_people: Optional[List[str]] = dspy.OutputField(\n",
    "        desc=\"Names of key individuals mentioned. Use names of famous individuals in their most recognized forms.\"\n",
    "    )\n",
    "    \n",
    "    mentioned_organizations: Optional[List[str]] = dspy.OutputField(\n",
    "        desc=\"Names of key organizations mentioned.\"\n",
    "    )\n",
    "    \n",
    "    mentioned_legislation: Optional[List[str]] = dspy.OutputField(\n",
    "        desc=\"Laws, bills, or policies mentioned.\"\n",
    "    )\n",
    "        \n",
    "    mentioned_locations: Optional[List[str]] = dspy.OutputField(\n",
    "        desc=\"Geographic locations named in the article.\"\n",
    "    )\n",
    "\n",
    "    # SENTIMENT ANALYSIS\n",
    "    sentiment_tone: Optional[Literal[\"positive\", \"neutral\", \"negative\"]] = dspy.OutputField(\n",
    "        desc=\"Overall sentiment expressed in the article.\"\n",
    "    )\n",
    "    \n",
    "    # ADDITIONAL CONTEXTUAL INFORMATION\n",
    "    extracted_quotes: Optional[List[str]] = dspy.OutputField(\n",
    "        desc=\"Notable direct quotes from the article.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "NewsAppSignature.__doc__ =  \"\"\"\n",
    "You are provided with the text of a news article.\n",
    "Help provide the requested information for cataloging and retrieval.\n",
    "Ensure information is focused on quality retrieval results -- accuracy, specificity, disambiguation.\n",
    "Correct simple grammar or formatting mistakes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cf27a-ab1e-4d04-b478-5dbd1fbd0b3f",
   "metadata": {},
   "source": [
    "# Run the program\n",
    "\n",
    "I like the natural code style of writing a DSPy signature. A pydantic model becomes the prompt.\n",
    "\n",
    "`Literal` type + LLM = classifier (cool!)\n",
    "\n",
    "We can already try it out, using the ChainOfThought predictor to run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb03ce3-c352-4510-91b3-e600a885cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Business Briefing Dec. 2, 2015\n",
    "Nokia shareholders overwhelmingly approved the acquisition of the ailing French telecom Alcatel-Lucent, removing one of the last hurdles to a 15.6 billion euro ($16.5 billion) deal that will make Nokia a market leader in networks.\n",
    "In October, Nokia said it would pay 4 billion to shareholders as the company raised its outlook for the year.\n",
    "Rajeev Suri, Nokias chief executive, said he was delighted by shareholders recognizing the long-term value creation opportunity of the deal, which is expected to close during the first quarter of 2016.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a08ef1-b89a-43a9-a184-7397b298dce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning=\"The article discusses Nokia's acquisition of Alcatel-Lucent, highlighting shareholder approval, financial details, and the expected closing date of the deal. The tone is positive, focusing on the long-term value creation opportunity. The primary category is business, and the content type is reporting. Key individuals and organizations are mentioned, along with financial figures and a timeline.\",\n",
      "    generated_title='Nokia Shareholders Approve Acquisition of Alcatel-Lucent in €15.6 Billion Deal',\n",
      "    publication_date='2015-12-02',\n",
      "    primary_category='business',\n",
      "    content_type='reporting',\n",
      "    keywords=['Nokia', 'Alcatel-Lucent', 'acquisition', 'shareholders', 'telecom', 'networks', 'Rajeev Suri', 'market leader'],\n",
      "    mentioned_people=['Rajeev Suri'],\n",
      "    mentioned_organizations=['Nokia', 'Alcatel-Lucent'],\n",
      "    mentioned_legislation=None,\n",
      "    mentioned_locations=['France'],\n",
      "    sentiment_tone='positive',\n",
      "    extracted_quotes=['Rajeev Suri, Nokias chief executive, said he was delighted by shareholders recognizing the long-term value creation opportunity of the deal, which is expected to close during the first quarter of 2016.']\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "catalog = dspy.ChainOfThought(NewsAppSignature)\n",
    "catalog_item = catalog(article_text=text)\n",
    "print(catalog_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfcf17-a0dd-43b4-94e1-534769e3ecb2",
   "metadata": {},
   "source": [
    "# Optimize it\n",
    "\n",
    "Good -- now we just need to load some example data...\n",
    "\n",
    "\n",
    "\n",
    "## A basic test time scaling\n",
    "\n",
    "I'll generate some training data using a simplistic best-of-n style test time scaling. Aggregating all of the types is a bit more challenging, so I've done that in the `aggregate/` folder as a module that I can work on further.\n",
    "\n",
    "Depending on where you are running your LLM calls, you might choose the serial or parallel methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fca1d4-995b-4e23-8cae-f95cf0bff7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import Parallel, ChainOfThought\n",
    "from typing import List, Literal\n",
    "import tqdm\n",
    "\n",
    "def generate_candidates_serial(text, n=8):\n",
    "    \"\"\" Run iteratively \"\"\"\n",
    "    return [catalog(article_text=text) for _ in range(n)]\n",
    "\n",
    "\n",
    "def generate_candidates_parallel(text, n=8, num_threads=2):\n",
    "    \"\"\" Run in parallel \"\"\"\n",
    "    parallel_executor = dspy.Parallel(num_threads=num_threads)\n",
    "    exec_pairs = [(catalog, {'article_text': text}) for _ in range(n)]\n",
    "    results = parallel_executor.forward(exec_pairs)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35393f63-d9ee-4b6f-aab7-da90e213d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from aggregate.aggregate import LLMOutputAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28246978-1f09-4209-95ce-4c17035b1bff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### aggregate\n",
    "\n",
    "prototyping... moved to module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6798a-765f-4bc0-9e55-dbe321970efe",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import List, Optional, Literal, Dict, Any, Union\n",
    "from collections import Counter\n",
    "import textdistance\n",
    "import itertools\n",
    "from pydantic import ValidationError\n",
    "from typing import get_origin, get_args, Union\n",
    "\n",
    "def is_optional_field(type_hint) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if a type hint is Optional, i.e., Union[X, None].\n",
    "    \"\"\"\n",
    "    return get_origin(type_hint) is Union and type(None) in get_args(type_hint)\n",
    "\n",
    "def aggregate_signatures(text, predictions: List[Any], threshold: int = 2, debug: bool = False) -> NewsAppSignature:\n",
    "    \"\"\"\n",
    "    Aggregates multiple Prediction objects into a single NewsAppSignature.\n",
    "    \n",
    "    Args:\n",
    "        predictions (List[Any]): A list of Prediction objects.\n",
    "        threshold (int): Minimum number of occurrences for a cluster to be included.\n",
    "    \n",
    "    Returns:\n",
    "        NewsAppSignature: The aggregated signature.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If required fields are missing or validation fails.\n",
    "    \"\"\"\n",
    "    if not predictions:\n",
    "        raise ValueError(\"No predictions to aggregate.\")\n",
    "\n",
    "    aggregated_fields: Dict[str, Any] = {}\n",
    "    \n",
    "    # Helper function for majority voting\n",
    "    def majority_vote(values: List[Any]) -> Any:\n",
    "        counter = Counter(values)\n",
    "        if debug:\n",
    "            print(counter)\n",
    "        most_common, count = counter.most_common(1)[0]\n",
    "        return most_common\n",
    "\n",
    "    # Helper function for clustering similar strings with frequency threshold\n",
    "    def cluster_strings_with_threshold(strings: List[str], threshold: int, similarity_threshold: float = 0.6) -> List[str]:\n",
    "        \"\"\"\n",
    "        Clusters similar strings based on Jaccard similarity and filters clusters based on frequency threshold.\n",
    "        \n",
    "        Args:\n",
    "            strings (List[str]): List of strings to cluster.\n",
    "            threshold (int): Minimum number of occurrences for a cluster to be included.\n",
    "            similarity_threshold (float): Jaccard similarity threshold for clustering.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of representative strings from clusters that meet the threshold.\n",
    "        \"\"\"\n",
    "        clusters = []\n",
    "        for string in strings:\n",
    "            added = False\n",
    "            for cluster in clusters:\n",
    "                # Compare with the first item in the cluster\n",
    "                similarity = textdistance.jaccard.normalized_similarity(\n",
    "                    set(string.lower().split()), set(cluster[0].lower().split())\n",
    "                )\n",
    "                if similarity >= similarity_threshold:\n",
    "                    cluster.append(string)\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                clusters.append([string])\n",
    "        \n",
    "        # Filter clusters based on threshold\n",
    "        if debug:\n",
    "            print(clusters)\n",
    "        filtered_clusters = [cluster for cluster in clusters if len(cluster) >= threshold]\n",
    "        \n",
    "        # Return one representative from each filtered cluster\n",
    "        return [cluster[0] for cluster in filtered_clusters]\n",
    "    \n",
    "    # Iterate over each field in the NewsAppSignature\n",
    "    for field_name, field_type in NewsAppSignature.__annotations__.items():\n",
    "        # Special handling for 'article_text' since it's turned off\n",
    "        if field_name == \"article_text\":\n",
    "            # Set 'article_text' to an empty string as per user's instruction\n",
    "            aggregated_fields[field_name] = text\n",
    "            continue\n",
    "\n",
    "        # Collect all non-None values for the current field\n",
    "        field_values = [getattr(pred, field_name, None) for pred in predictions]\n",
    "        field_values = [val for val in field_values if val is not None]\n",
    "\n",
    "        if not field_values:\n",
    "            # Determine if the field is optional\n",
    "            if is_optional_field(field_type):\n",
    "                aggregated_fields[field_name] = None\n",
    "            else:\n",
    "                # For required fields with no values, raise an error\n",
    "                raise ValueError(f\"No values found for required field '{field_name}' during aggregation.\")\n",
    "            continue\n",
    "\n",
    "        # Determine the field type for aggregation\n",
    "        origin_type = get_origin(field_type)\n",
    "        args_type = get_args(field_type)\n",
    "\n",
    "        # Handle Literal types\n",
    "        if origin_type is Literal:\n",
    "            # Majority voting for Literal fields\n",
    "            aggregated_fields[field_name] = majority_vote(field_values)\n",
    "        elif isinstance(field_values[0], str):\n",
    "            # Majority voting for single-string fields\n",
    "            aggregated_fields[field_name] = majority_vote(field_values)\n",
    "        elif isinstance(field_values[0], list):\n",
    "            # Flatten all lists\n",
    "            flattened = list(itertools.chain.from_iterable(field_values))\n",
    "            # Cluster similar strings with frequency threshold\n",
    "            clustered = cluster_strings_with_threshold(flattened, threshold=threshold)\n",
    "            aggregated_fields[field_name] = clustered\n",
    "        else:\n",
    "            # Handle other types if necessary\n",
    "            aggregated_fields[field_name] = majority_vote(field_values)\n",
    "\n",
    "    # Instantiate the aggregated NewsAppSignature with all fields\n",
    "    try:\n",
    "        aggregated_signature = NewsAppSignature(**aggregated_fields)\n",
    "    except ValidationError as ve:\n",
    "        # Extract detailed validation errors\n",
    "        raise ValueError(f\"Error creating aggregated NewsAppSignature: {ve}\")\n",
    "\n",
    "    return aggregated_signature\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319c68b-3229-4e5a-b46d-ce345d6fcd56",
   "metadata": {},
   "source": [
    "```python\n",
    "consensus = aggregate_signatures(text, results, debug=True, threshold=4)\n",
    "consensus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c8957-bdd1-4256-b115-24f975d87f47",
   "metadata": {},
   "source": [
    "## Process a bunch of data\n",
    "\n",
    "We can load `ag_news` to create our synthetic training data, and process ~100 rows.\n",
    "\n",
    "I'll save the save the results as I go. Quick and dirty, just restart if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7694045-7294-497f-bc2f-db4a39076e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a diverse news dataset (e.g., \"ag_news\")\n",
    "dataset = load_dataset(\"valurank/News_Articles_Categorization\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c166f1c-68fe-46a8-8a9f-5f00cac3716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "# Define the number of articles and samples\n",
    "num_articles = 100\n",
    "samples_per_article = 8\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"training_data\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define a file to keep track of progress (offset)\n",
    "progress_file = os.path.join(output_dir, \"progress.txt\")\n",
    "\n",
    "# Function to generate a non-cryptographic hash (e.g., MD5) of a JSON string\n",
    "def generate_hash(json_str: str) -> str:\n",
    "    return hashlib.md5(json_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Function to load the current offset\n",
    "def load_offset() -> int:\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            try:\n",
    "                offset = int(f.read().strip())\n",
    "                return offset\n",
    "            except ValueError:\n",
    "                return 0\n",
    "    return 0\n",
    "\n",
    "# Function to save the current offset\n",
    "def save_offset(offset: int):\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(str(offset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c3542-83ae-4782-8a85-663ad055c128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:   0%|                                                                                                                      | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|                                                                                                                                            | 0/8 [00:00<?, ?it/s]\n",
      "\u001b[Acessed 1 / 8 examples:   0%|                                                                                                                  | 0/8 [00:12<?, ?it/s]\n",
      "\u001b[Acessed 1 / 8 examples:  12%|█████████████▎                                                                                            | 1/8 [00:12<01:27, 12.57s/it]\n",
      "\u001b[Acessed 2 / 8 examples:  12%|█████████████▎                                                                                            | 1/8 [00:24<01:27, 12.57s/it]\n",
      "\u001b[Acessed 2 / 8 examples:  25%|██████████████████████████▌                                                                               | 2/8 [00:24<01:13, 12.20s/it]\n",
      "\u001b[Acessed 3 / 8 examples:  25%|██████████████████████████▌                                                                               | 2/8 [00:37<01:13, 12.20s/it]\n",
      "\u001b[Acessed 3 / 8 examples:  38%|███████████████████████████████████████▊                                                                  | 3/8 [00:37<01:03, 12.65s/it]\n",
      "\u001b[Acessed 4 / 8 examples:  38%|███████████████████████████████████████▊                                                                  | 3/8 [00:49<01:03, 12.65s/it]\n",
      "\u001b[Acessed 4 / 8 examples:  50%|█████████████████████████████████████████████████████                                                     | 4/8 [00:49<00:49, 12.35s/it]\n",
      "\u001b[Acessed 5 / 8 examples:  50%|█████████████████████████████████████████████████████                                                     | 4/8 [00:59<00:49, 12.35s/it]\n",
      "\u001b[Acessed 5 / 8 examples:  62%|██████████████████████████████████████████████████████████████████▎                                       | 5/8 [00:59<00:34, 11.49s/it]\n",
      "\u001b[Acessed 6 / 8 examples:  62%|██████████████████████████████████████████████████████████████████▎                                       | 5/8 [01:01<00:34, 11.49s/it]\n",
      "\u001b[Acessed 6 / 8 examples:  75%|███████████████████████████████████████████████████████████████████████████████▌                          | 6/8 [01:01<00:16,  8.34s/it]\n",
      "\u001b[Acessed 7 / 8 examples:  75%|███████████████████████████████████████████████████████████████████████████████▌                          | 6/8 [01:22<00:16,  8.34s/it]\n",
      "\u001b[Acessed 7 / 8 examples:  88%|████████████████████████████████████████████████████████████████████████████████████████████▊             | 7/8 [01:22<00:12, 12.45s/it]\n",
      "\u001b[Acessed 8 / 8 examples:  88%|████████████████████████████████████████████████████████████████████████████████████████████▊             | 7/8 [01:24<00:12, 12.45s/it]\n",
      "Processed 8 / 8 examples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [01:24<00:00, 10.59s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:   1%|█                                                                                                           | 1/97 [01:25<2:16:09, 85.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A%|                                                                                                                                            | 0/8 [00:00<?, ?it/s]\n",
      "\u001b[Acessed 1 / 8 examples:   0%|                                                                                                                  | 0/8 [00:38<?, ?it/s]\n",
      "\u001b[Acessed 1 / 8 examples:  12%|█████████████▎                                                                                            | 1/8 [00:38<04:28, 38.33s/it]\n",
      "\u001b[Acessed 2 / 8 examples:  12%|█████████████▎                                                                                            | 1/8 [00:58<04:28, 38.33s/it]\n",
      "\u001b[Acessed 2 / 8 examples:  25%|██████████████████████████▌                                                                               | 2/8 [00:58<02:45, 27.64s/it]\n",
      "\u001b[Acessed 3 / 8 examples:  25%|██████████████████████████▌                                                                               | 2/8 [01:10<02:45, 27.64s/it]\n",
      "\u001b[Acessed 3 / 8 examples:  38%|███████████████████████████████████████▊                                                                  | 3/8 [01:10<01:42, 20.57s/it]"
     ]
    }
   ],
   "source": [
    "# Initialize the starting offset\n",
    "start_offset = load_offset()\n",
    "\n",
    "# Iterate over the specified number of articles starting from the offset\n",
    "for i in tqdm.tqdm(range(start_offset, num_articles), desc=\"Processing Articles\", total=num_articles - start_offset):\n",
    "    try:\n",
    "        article = dataset[i]\n",
    "        text = article['Text']\n",
    "        \n",
    "        # Generate multiple predictions\n",
    "        # candidates = generate_candidates_serial(text, n=samples_per_article)\n",
    "        candidates = generate_candidates_parallel(text, n=samples_per_article)\n",
    "        \n",
    "        # Aggregate predictions to form consensus\n",
    "        candidates_with_text = []\n",
    "        for c in [c.toDict() for c in candidates]:\n",
    "            c.update({\"article_text\": text})\n",
    "            candidates_with_text.append(c)\n",
    "        candidates_with_text\n",
    "        consensus = LLMOutputAggregator.aggregate(\n",
    "            NewsAppSignature, candidates_with_text, threshold=3\n",
    "        )\n",
    "        \n",
    "        # Convert consensus to JSON string\n",
    "        consensus_json = consensus.model_dump_json()\n",
    "        \n",
    "        # Generate filename using hash of JSON string\n",
    "        filename_hash = generate_hash(consensus_json)\n",
    "        filename = f\"{filename_hash}.json\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the JSON string to the file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(consensus_json)\n",
    "        \n",
    "        # Update the progress offset\n",
    "        save_offset(i + 1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article {i}: {e}\")\n",
    "        # Optionally, log the error to a file\n",
    "        error_log = os.path.join(output_dir, \"error_log.txt\")\n",
    "        with open(error_log, 'a') as f:\n",
    "            f.write(f\"Article {i}: {e}\\n\")\n",
    "        # Continue with the next article\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6077c-ec7a-45c9-9e73-d00d1ef2d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mipro_optimizer = dspy.MIPROv2(\n",
    "#     metric=extraction_correctness_metric,\n",
    "#     auto=\"medium\",\n",
    "# )\n",
    "# optimized_people_extractor = mipro_optimizer.compile(\n",
    "#     people_extractor,\n",
    "#     trainset=train_set,\n",
    "#     max_bootstrapped_demos=4,\n",
    "#     requires_permission_to_run=False,\n",
    "#     minibatch=False\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
